import sys
import os
import asyncio
import json
from typing import AsyncGenerator, List, Dict, Any

import gradio as gr
from rich.console import Console
from rich.text import Text

from augmented.chat_openai import AsyncChatOpenAI

base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(base_dir)

from augmented.agent import Agent  # ç›´æ¥å¤ç”¨ä½ çš„ Agent ç±»
from augmented.mcp_client import MCPClient
from augmented.mcp_tools import PresetMcpTools
from augmented.utils.info import (
    PROJECT_ROOT_DIR,
    DEFAULT_MODEL_NAME
)
from augmented.utils import pretty
from augmented.utils.info import DEFAULT_MODEL_NAME, PROJECT_ROOT_DIR

PRETTY_LOGGER = pretty.ALogger("[Agent]")

# å…¨å±€çŠ¶æ€ï¼šä¿å­˜å¯¹è¯å†å²å’Œ Agent å®ä¾‹ï¼ˆå¤šè½®å¯¹è¯æ”¯æŒï¼‰
class State:
    def __init__(self):
        self.agent: Agent | None = None
        self.chat_history: List[Dict[str, str]] = []  # å¤šè½®å¯¹è¯å†å²
        self.console = Console(record=True)  # æ•è· rich è¾“å‡º


state = State()


async def init_agent_if_needed(model_name: str, base_url: str, api_key: str) -> Agent:
    """åˆå§‹åŒ– Agentï¼ˆä»…åœ¨é¦–æ¬¡æˆ–æ¨¡å‹å‚æ•°å˜åŒ–æ—¶ï¼‰"""
    if state.agent is None:
        # å¤ç”¨ä½ çš„ MCP å®¢æˆ·ç«¯é…ç½®ï¼ˆå’Œ example() å®Œå…¨ä¸€è‡´ï¼‰
        mcp_clients = []
        for mcp_tool in [
            PresetMcpTools.filesystem.append_mcp_params(f" {PROJECT_ROOT_DIR!s}"),
            PresetMcpTools.fetch,
            PresetMcpTools.commander
        ]:
            mcp_client = MCPClient(**mcp_tool.to_common_params())
            mcp_clients.append(mcp_client)

        # å®Œå…¨å¤ç”¨ä½ çš„ Agent åˆå§‹åŒ–é€»è¾‘ï¼ˆåŒ…æ‹¬ system_promptï¼‰
        state.agent = Agent(
            model=model_name or DEFAULT_MODEL_NAME,
            mcp_clients=mcp_clients,
        )
        # åˆå§‹åŒ– LLMï¼ˆä¿ç•™ä½ çš„ system_promptï¼‰
        state.agent.llm = AsyncChatOpenAI(
            model_name or DEFAULT_MODEL_NAME,
            tools=[],  # ç”± agent.init() è‡ªåŠ¨å¡«å……
            system_prompt=state.agent.system_prompt,  # å…³é”®ï¼šä¿ç•™ä½ çš„ system prompt
            context=state.agent.context,
        )
        await state.agent.init()
        PRETTY_LOGGER.title("Agent åˆå§‹åŒ–å®Œæˆ")
    return state.agent


async def gradio_query(
        query: str,
        model_name: str,
        base_url: str,
        api_key: str,
        temperature: float,
        chat_history: List[Dict[str, str]]
) -> AsyncGenerator[tuple[str, str, List[Dict[str, str]]], None]:
    """å®Œå…¨å¤ç”¨ Agent çš„å¤šè½®å¯¹è¯å’Œè¾“å‡ºé€»è¾‘ï¼Œåˆ†ç¦»å·¥å…·è°ƒç”¨å’Œæœ€ç»ˆå“åº”"""
    tool_call_text = ""  # å·¦ä¾§å·¥å…·è°ƒç”¨è¯¦æƒ…
    response_text = ""  # å³ä¾§æœ€ç»ˆå“åº”å†…å®¹

    try:
        # 1. åˆå§‹åŒ– Agent
        agent = await init_agent_if_needed(model_name, base_url, api_key)
        agent.llm.temperature = temperature
        tool_call_text += "åˆå§‹åŒ–å®Œæˆï¼Œå¼€å§‹å¤„ç†æŸ¥è¯¢...\n"
        yield tool_call_text, response_text, chat_history

        # 2. è®°å½•ç”¨æˆ·æŸ¥è¯¢åˆ°å†å²
        chat_history.append({"role": "user", "content": query})

        # 3. å¤„ç†å¤šè½®å·¥å…·è°ƒç”¨
        chat_resp = await agent.llm.chat(query)
        i = 0

        while True:
            i += 1
            tool_call_text += f"\n=== å¤„ç†è½®æ¬¡ {i} ===\n"

            # å¤„ç†å·¥å…·è°ƒç”¨ï¼ˆåªåœ¨å·¦ä¾§æ˜¾ç¤ºï¼‰
            if chat_resp.tool_calls:
                for tool_call in chat_resp.tool_calls:
                    tool_info = f"ğŸ› ï¸ TOOL USE: {tool_call.function.name}\n  ARGS: {tool_call.function.arguments}\n"
                    tool_call_text += tool_info
                    yield tool_call_text, response_text, chat_history

                    target_mcp_client = next(
                        (c for c in agent.mcp_clients if tool_call.function.name in [t.name for t in c.get_tools()]),
                        None
                    )

                    if target_mcp_client:
                        mcp_result = await target_mcp_client.call_tool(
                            tool_call.function.name,
                            json.loads(tool_call.function.arguments),
                        )
                        tool_call_text += f"âœ… RESULT: {str(mcp_result)}\n\n"
                        yield tool_call_text, response_text, chat_history
                        agent.llm.append_tool_result(tool_call.id, mcp_result.model_dump_json())
                    else:
                        tool_call_text += f"âŒ é”™è¯¯: å·¥å…·æœªæ‰¾åˆ°\n"
                        yield tool_call_text, response_text, chat_history
                        return

                # ç»§ç»­ä¸‹ä¸€è½®å¯¹è¯
                chat_resp = await agent.llm.chat()
            else:
                # 4. æœ€ç»ˆå“åº”ï¼ˆåªåœ¨å³ä¾§æ˜¾ç¤ºï¼‰
                response_text = chat_resp.content
                chat_history.append({"role": "assistant", "content": response_text})
                yield tool_call_text, response_text, chat_history
                break

    except Exception as e:
        error_msg = f"âŒ å¤„ç†å¤±è´¥: {str(e)}"
        response_text = error_msg
        yield tool_call_text, response_text, chat_history
    finally:
        pass








# async def gradio_query(
#         query: str,
#         model_name: str,
#         base_url: str,
#         api_key: str,
#         temperature: float,
#         chat_history: List[Dict[str, str]]
# ) -> AsyncGenerator[tuple[str, str, List[Dict[str, str]]], None]:
#     """å®Œå…¨å¤ç”¨ Agent çš„å¤šè½®å¯¹è¯å’Œè¾“å‡ºé€»è¾‘"""
#     tool_call_text = ""
#     full_response = ""
#     state.console.clear()  # æ¸…ç©º rich æ§åˆ¶å°ç¼“å­˜
#
#     try:
#         # 1. åˆå§‹åŒ– Agentï¼ˆå¤ç”¨ä½ çš„æ ¸å¿ƒé€»è¾‘ï¼‰
#         agent = await init_agent_if_needed(model_name, base_url, api_key)
#         agent.llm.temperature = temperature  # è®¾ç½®æ¸©åº¦
#         yield tool_call_text, "åˆå§‹åŒ–å®Œæˆï¼Œå¼€å§‹å¤„ç†æŸ¥è¯¢...", chat_history
#
#         # 2. ç»´æŠ¤å¤šè½®å¯¹è¯å†å²ï¼ˆæ–°å¢ï¼‰
#         state.chat_history = chat_history.copy()
#         state.chat_history.append({"role": "user", "content": query})
#
#         # 3. å¤ç”¨ Agent çš„å¤šè½®å·¥å…·è°ƒç”¨é€»è¾‘ï¼ˆæ”¹é€ ä¸ºæµå¼è¾“å‡ºï¼‰
#         chat_resp = await agent.llm.chat(query)  # é¦–æ¬¡è°ƒç”¨
#         i = 0
#
#         while True:
#             # è¾“å‡ºè½®æ¬¡ä¿¡æ¯ï¼ˆå’Œä½ çš„ Agent è¾“å‡ºä¸€è‡´ï¼‰
#             è½®æ¬¡ä¿¡æ¯ = f"\n=== å¤„ç†è½®æ¬¡ {i} ==="
#             full_response += è½®æ¬¡ä¿¡æ¯
#             yield tool_call_text, full_response, state.chat_history
#             i += 1
#
#             # å¤„ç†å·¥å…·è°ƒç”¨ï¼ˆå®Œå…¨å¤ç”¨ä½ çš„é€»è¾‘ï¼‰
#             if chat_resp.tool_calls:
#                 for tool_call in chat_resp.tool_calls:
#                     # è¾“å‡ºå·¥å…·è°ƒç”¨ä¿¡æ¯ï¼ˆå’Œä½ çš„ rprint ä¸€è‡´ï¼‰
#                     tool_info = f"TOOL USE `{tool_call.function.name}`\nwith args: {tool_call.function.arguments}"
#                     tool_call_text += tool_info + "\n"
#                     full_response += f"\nå·¥å…·è°ƒç”¨: {tool_call.function.name}\n"
#                     yield tool_call_text, full_response, state.chat_history
#
#                     # æŸ¥æ‰¾ç›®æ ‡ MCP å®¢æˆ·ç«¯ï¼ˆå¤ç”¨ä½ çš„é€»è¾‘ï¼‰
#                     target_mcp_client = next(
#                         (c for c in agent.mcp_clients if tool_call.function.name in [t.name for t in c.get_tools()]),
#                         None
#                     )
#
#                     if target_mcp_client:
#                         # è°ƒç”¨å·¥å…·ï¼ˆå¤ç”¨ä½ çš„ call_toolï¼‰
#                         mcp_result = await target_mcp_client.call_tool(
#                             tool_call.function.name,
#                             json.loads(tool_call.function.arguments),
#                         )
#                         # è¾“å‡ºå·¥å…·è¿”å›ç»“æœï¼ˆå’Œä½ çš„ rprint ä¸€è‡´ï¼‰
#                         result_str = f"å·¥å…·è¿”å›: {str(mcp_result)}\n"
#                         full_response += result_str
#                         tool_call_text += result_str
#                         yield tool_call_text, full_response, state.chat_history
#
#                         # è®°å½•å·¥å…·ç»“æœï¼ˆå¤ç”¨ä½ çš„ append_tool_resultï¼‰
#                         agent.llm.append_tool_result(tool_call.id, mcp_result.model_dump_json())
#                     else:
#                         error = "å·¥å…·æœªæ‰¾åˆ°"
#                         full_response += f"\né”™è¯¯: {error}\n"
#                         yield tool_call_text, full_response, state.chat_history
#                         return
#
#                 # å¤šè½®å¯¹è¯ï¼šç»§ç»­è°ƒç”¨ LLMï¼ˆå¤ç”¨ä½ çš„é€»è¾‘ï¼‰
#                 chat_resp = await agent.llm.chat()
#             else:
#                 # è¾“å‡ºæœ€ç»ˆç»“æœï¼ˆå’Œä½ çš„è¿”å›ä¸€è‡´ï¼‰
#                 final_resp = f"\næœ€ç»ˆç»“æœ: {chat_resp.content}"
#                 full_response += final_resp
#                 state.chat_history.append({"role": "assistant", "content": chat_resp.content})
#                 yield tool_call_text, full_response, state.chat_history
#                 break
#
#     except Exception as e:
#         # é”™è¯¯å¤„ç†ï¼ˆå¤ç”¨ä½ çš„å¼‚å¸¸è¾“å‡ºï¼‰
#         error_msg = f"å¤„ç†å¤±è´¥: {str(e)}"
#         yield tool_call_text, error_msg, state.chat_history
#     finally:
#         # ä¿ç•™ä½ çš„ cleanup é€»è¾‘ï¼ˆä¸æ¸…ç†ï¼Œæ”¯æŒå¤šè½®å¯¹è¯ï¼‰
#         pass

# åˆå§‹åŒ– rich æ§åˆ¶å°ï¼ˆå¤ç”¨ä½ çš„è¾“å‡ºæ ¼å¼ï¼‰
state.console = Console()

# Gradio ç•Œé¢ï¼ˆä¿ç•™ä½ çš„è¾“å‡ºé£æ ¼ï¼‰
with gr.Blocks(title="MCP Agent äº¤äº’ç•Œé¢") as demo:
    gr.Markdown("## ğŸ¤– MCP Agent äº¤äº’å¹³å°ï¼ˆå¤ç”¨åŸå§‹é€»è¾‘ï¼‰")

    # å¤šè½®å¯¹è¯å†å²çŠ¶æ€ï¼ˆæ–°å¢ï¼‰
    chat_history = gr.State([])

    with gr.Row():
        # å·¦ä¾§å‚æ•°åŒº
        with gr.Column(scale=1):
            gr.Markdown("### ğŸ§  æ¨¡å‹é…ç½®")
            model_name = gr.Textbox(label="æ¨¡å‹åç§°", value=DEFAULT_MODEL_NAME)
            base_url = gr.Textbox(label="API åœ°å€", value=os.environ.get("OPENAI_BASE_URL"))
            api_key = gr.Textbox(label="API Key", type="password", value=os.environ.get("OPENAI_API_KEY"))
            temperature = gr.Slider(label="æ¸©åº¦", minimum=0.0, maximum=1.0, value=0.0, step=0.1)  # ä½ çš„é»˜è®¤æ¸©åº¦æ˜¯ 0

            # å·¥å…·è°ƒç”¨è®°å½•ï¼ˆå¤ç”¨ä½ çš„è¾“å‡ºï¼‰
            tool_status = gr.Textbox(label="ğŸ› ï¸ å·¥å…·è°ƒç”¨è¯¦æƒ…", lines=10, interactive=False)

        # å³ä¾§è¾“å‡ºåŒºï¼ˆä¿ç•™ä½ çš„è¾“å‡ºæ ¼å¼ï¼‰
        with gr.Column(scale=2):
            gr.Markdown("### ğŸ’¬ è¾“å‡ºç»“æœï¼ˆä¸ Agent åŸç”Ÿè¾“å‡ºä¸€è‡´ï¼‰")
            result_display = gr.Textbox(label="ç”Ÿæˆå†…å®¹", lines=20, show_copy_button=True)

    # åº•éƒ¨è¾“å…¥åŒº + å¤šè½®å¯¹è¯
    with gr.Row():
        query_input = gr.Textbox(label="â“ è¾“å…¥æŸ¥è¯¢", placeholder="è¯·è¾“å…¥ä½ çš„é—®é¢˜...", scale=4)
        generate_btn = gr.Button("ğŸš€ å¼€å§‹å¤„ç†", scale=1, variant="primary")

    # ç»‘å®šäº¤äº’é€»è¾‘ï¼ˆå¤šè½®å¯¹è¯ + æµå¼è¾“å‡ºï¼‰
    generate_btn.click(
        fn=gradio_query,
        inputs=[query_input, model_name, base_url, api_key, temperature, chat_history],
        outputs=[tool_status, result_display, chat_history]
    )

if __name__ == "__main__":
    demo.queue().launch(server_name="localhost", server_port=9999, auth=("zhangsan", "123456"))